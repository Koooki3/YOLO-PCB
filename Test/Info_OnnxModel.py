#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Info_OnnxModel.py - ONNXæ¨¡å‹ä¿¡æ¯åˆ†æå’Œå¯è§†åŒ–å·¥å…·

åŠŸèƒ½ï¼š
1. åˆ†æONNXæ¨¡å‹çš„è¾“å…¥è¾“å‡ºä¿¡æ¯
2. æµ‹è¯•æ¨ç†å¹¶ç”Ÿæˆå¯è§†åŒ–ç»“æœ
3. æä¾›C++ä»£ç éƒ¨ç½²çš„å‚è€ƒä¿¡æ¯

ä½¿ç”¨æ–¹æ³•ï¼š
python Info_OnnxModel.py --model path/to/model.onnx --image path/to/test.jpg

ä½œè€…ï¼šClaude
æ—¥æœŸï¼š2024
"""

import onnxruntime as ort
import numpy as np
import cv2
import argparse
import json
import sys
import os
from typing import Dict, List, Tuple, Any
import matplotlib.pyplot as plt
import matplotlib.patches as patches

class ONNXModelAnalyzer:
    """ONNXæ¨¡å‹åˆ†æå™¨"""
    
    def __init__(self, model_path: str):
        """
        åˆå§‹åŒ–æ¨¡å‹åˆ†æå™¨
        
        Args:
            model_path: ONNXæ¨¡å‹æ–‡ä»¶è·¯å¾„
        """
        self.model_path = model_path
        self.session = None
        self.input_info = {}
        self.output_info = {}
        self.input_names = []
        self.output_names = []
        
        # åŠ è½½æ¨¡å‹
        self._load_model()
        
    def _load_model(self):
        """åŠ è½½ONNXæ¨¡å‹"""
        try:
            self.session = ort.InferenceSession(self.model_path)
            self.input_names = [input.name for input in self.session.get_inputs()]
            self.output_names = [output.name for output in self.session.get_outputs()]
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_path}")
            print(f"ğŸ“Š è¾“å…¥èŠ‚ç‚¹æ•°é‡: {len(self.input_names)}")
            print(f"ğŸ“Š è¾“å‡ºèŠ‚ç‚¹æ•°é‡: {len(self.output_names)}")
            
            # åˆ†æè¾“å…¥è¾“å‡ºä¿¡æ¯
            self._analyze_io_info()
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _analyze_io_info(self):
        """åˆ†æè¾“å…¥è¾“å‡ºä¿¡æ¯"""
        # åˆ†æè¾“å…¥
        for input_info in self.session.get_inputs():
            self.input_info[input_info.name] = {
                'shape': input_info.shape,
                'dtype': str(input_info.type),
                'name': input_info.name
            }
        
        # åˆ†æè¾“å‡º
        for output_info in self.session.get_outputs():
            self.output_info[output_info.name] = {
                'shape': output_info.shape,
                'dtype': str(output_info.type),
                'name': output_info.name
            }
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹åŸºæœ¬ä¿¡æ¯"""
        return {
            'model_path': self.model_path,
            'input_nodes': self.input_info,
            'output_nodes': self.output_info,
            'input_names': self.input_names,
            'output_names': self.output_names,
            'providers': self.session.get_providers()
        }
    
    def preprocess_image(self, image_path: str, target_size: Tuple[int, int] = None) -> np.ndarray:
        """
        å›¾åƒé¢„å¤„ç†
        
        Args:
            image_path: å›¾åƒè·¯å¾„
            target_size: ç›®æ ‡å°ºå¯¸ (width, height)
            
        Returns:
            é¢„å¤„ç†åçš„å›¾åƒnumpyæ•°ç»„
        """
        # è¯»å–å›¾åƒ
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {image_path}")
        
        original_height, original_width = image.shape[:2]
        print(f"ğŸ“· åŸå§‹å›¾åƒå°ºå¯¸: {original_width}x{original_height}")
        
        # è°ƒæ•´å°ºå¯¸
        if target_size is None:
            # ä½¿ç”¨æ¨¡å‹çš„é»˜è®¤è¾“å…¥å°ºå¯¸
            input_shape = self.input_info[self.input_names[0]]['shape']
            if len(input_shape) == 4:  # NCHW or NHWC
                target_size = (input_shape[3], input_shape[2])  # (width, height)
            else:
                target_size = (640, 640)  # é»˜è®¤å°ºå¯¸
        
        # ç¼©æ”¾å›¾åƒ
        resized = cv2.resize(image, target_size)
        
        # æ ‡å‡†åŒ– (å‡è®¾ä½¿ç”¨ImageNetæ ‡å‡†)
        normalized = resized.astype(np.float32) / 255.0
        
        # é€šé“è½¬æ¢ (BGR to RGB)
        rgb_image = cv2.cvtColor(normalized, cv2.COLOR_BGR2RGB)
        
        # è°ƒæ•´ç»´åº¦é¡ºåº (HWC to CHW)
        transposed = np.transpose(rgb_image, (2, 0, 1))
        
        # æ·»åŠ batchç»´åº¦
        batch_image = np.expand_dims(transposed, axis=0)
        
        print(f"ğŸ”§ é¢„å¤„ç†å®Œæˆï¼Œç›®æ ‡å°ºå¯¸: {target_size[0]}x{target_size[1]}")
        print(f"ğŸ“ æœ€ç»ˆå¼ é‡å½¢çŠ¶: {batch_image.shape}")
        
        return batch_image, (original_width, original_height)
    
    def run_inference(self, input_data: np.ndarray) -> List[np.ndarray]:
        """
        è¿è¡Œæ¨ç†
        
        Args:
            input_data: è¾“å…¥æ•°æ®
            
        Returns:
            è¾“å‡ºç»“æœåˆ—è¡¨
        """
        try:
            # æ„å»ºè¾“å…¥å­—å…¸
            inputs = {self.input_names[0]: input_data}
            
            # è¿è¡Œæ¨ç†
            outputs = self.session.run(None, inputs)
            
            print(f"ğŸš€ æ¨ç†å®Œæˆï¼Œè¾“å‡ºæ•°é‡: {len(outputs)}")
            
            return outputs
            
        except Exception as e:
            print(f"âŒ æ¨ç†å¤±è´¥: {e}")
            raise
    
    def analyze_yolo_output(self, outputs: List[np.ndarray]) -> Dict[str, Any]:
        """
        åˆ†æYOLOè¾“å‡º
        
        Args:
            outputs: æ¨¡å‹è¾“å‡º
            
        Returns:
            åˆ†æç»“æœ
        """
        analysis = {
            'num_outputs': len(outputs),
            'output_shapes': [],
            'detected_format': 'unknown',
            'suggested_postprocessing': {}
        }
        
        for i, output in enumerate(outputs):
            shape = output.shape
            analysis['output_shapes'].append({
                'output_index': i,
                'shape': shape,
                'dtype': str(output.dtype),
                'min_value': float(np.min(output)),
                'max_value': float(np.max(output)),
                'mean_value': float(np.mean(output))
            })
            
            print(f"ğŸ“Š è¾“å‡ºå±‚ {i}:")
            print(f"   å½¢çŠ¶: {shape}")
            print(f"   æ•°æ®ç±»å‹: {output.dtype}")
            print(f"   æ•°å€¼èŒƒå›´: {np.min(output):.6f} ~ {np.max(output):.6f}")
            print(f"   å¹³å‡å€¼: {np.mean(output):.6f}")
            
            # æ£€æµ‹YOLOæ ¼å¼
            if len(shape) == 3:
                if shape[1] < shape[2]:  # attributes < num_detections
                    analysis['detected_format'] = 'yolov8_format1'  # [batch, attributes, num_detections]
                    analysis['suggested_postprocessing'] = {
                        'format': 'YOLOv8 ONNXæ ¼å¼1',
                        'attributes': shape[1],
                        'detections': shape[2],
                        'description': '[batch, attributes, num_detections] - attributes: [x,y,w,h,obj_conf,class_0_conf,...]'
                    }
                elif shape[1] > shape[2]:  # attributes > num_detections
                    analysis['detected_format'] = 'yolov8_format2'  # [batch, num_detections, attributes]
                    analysis['suggested_postprocessing'] = {
                        'format': 'YOLOv8 ONNXæ ¼å¼2',
                        'detections': shape[1],
                        'attributes': shape[2],
                        'description': '[batch, num_detections, attributes] - attributes: [x,y,w,h,obj_conf,class_0_conf,...]'
                    }
            elif len(shape) == 2:
                analysis['detected_format'] = 'traditional_yolo'  # [rows, cols]
                analysis['suggested_postprocessing'] = {
                    'format': 'ä¼ ç»ŸYOLOæ ¼å¼',
                    'rows': shape[0],
                    'cols': shape[1],
                    'description': '[rows, cols] - æ¯è¡Œä¸€ä¸ªæ£€æµ‹ï¼ŒåŒ…å«[x,y,w,h,confidence,class_probs...]'
                }
        
        return analysis
    
    def visualize_results(self, image_path: str, outputs: List[np.ndarray], 
                         output_path: str = "result.jpg", conf_threshold: float = 0.5):
        """
        å¯è§†åŒ–æ£€æµ‹ç»“æœ
        
        Args:
            image_path: åŸå§‹å›¾åƒè·¯å¾„
            outputs: æ¨¡å‹è¾“å‡º
            output_path: è¾“å‡ºå›¾åƒè·¯å¾„
            conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        """
        # è¯»å–å¹¶é¢„å¤„ç†å›¾åƒ
        input_data, original_size = self.preprocess_image(image_path)
        
        # åˆ†æè¾“å‡º
        analysis = self.analyze_yolo_output(outputs)
        
        # è¯»å–åŸå§‹å›¾åƒ
        original_image = cv2.imread(image_path)
        if original_image is None:
            raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {image_path}")
        
        # åˆ›å»ºå¯è§†åŒ–å›¾åƒ
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # æ˜¾ç¤ºåŸå§‹å›¾åƒ
        ax1.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))
        ax1.set_title('åŸå§‹å›¾åƒ')
        ax1.axis('off')
        
        # è§£ææ£€æµ‹ç»“æœå¹¶ç»˜åˆ¶
        detections = self._parse_detections(outputs, analysis, original_size)
        
        # åœ¨å›¾åƒä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœ
        image_with_detections = original_image.copy()
        for detection in detections:
            if detection['confidence'] > conf_threshold:
                bbox = detection['bbox']
                label = f"{detection['class_name']} {detection['confidence']:.2f}"
                
                # ç»˜åˆ¶è¾¹ç•Œæ¡†
                cv2.rectangle(image_with_detections, 
                            (int(bbox[0]), int(bbox[1])), 
                            (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3])),
                            (0, 255, 0), 2)
                
                # ç»˜åˆ¶æ ‡ç­¾
                cv2.putText(image_with_detections, label,
                          (int(bbox[0]), int(bbox[1]) - 10),
                          cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        ax2.imshow(cv2.cvtColor(image_with_detections, cv2.COLOR_BGR2RGB))
        ax2.set_title(f'æ£€æµ‹ç»“æœ (é˜ˆå€¼: {conf_threshold})')
        ax2.axis('off')
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        print(f"ğŸ“Š å¯è§†åŒ–ç»“æœå·²ä¿å­˜: {output_path}")
        
        return detections, analysis
    
    def _parse_detections(self, outputs: List[np.ndarray], analysis: Dict, 
                         original_size: Tuple[int, int]) -> List[Dict]:
        """
        è§£ææ£€æµ‹ç»“æœ
        
        Args:
            outputs: æ¨¡å‹è¾“å‡º
            analysis: åˆ†æç»“æœ
            original_size: åŸå§‹å›¾åƒå°ºå¯¸
            
        Returns:
            è§£æåçš„æ£€æµ‹ç»“æœ
        """
        detections = []
        width, height = original_size
        
        if analysis['detected_format'] == 'yolov8_format1':
            # YOLOv8æ ¼å¼1: [batch, attributes, num_detections]
            output = outputs[0]  # å‡è®¾ç¬¬ä¸€ä¸ªè¾“å‡ºåŒ…å«æ£€æµ‹ç»“æœ
            batch_size, num_attributes, num_detections = output.shape
            
            for d in range(min(num_detections, 1000)):  # é™åˆ¶å¤„ç†æ•°é‡
                obj_conf = output[0, 4, d]  # å¯¹è±¡ç½®ä¿¡åº¦
                if obj_conf > 0.5:  # ç¬¬ä¸€å±‚è¿‡æ»¤
                    # æ‰¾åˆ°æœ€é«˜ç±»åˆ«ç½®ä¿¡åº¦
                    class_id = -1
                    max_class_conf = 0
                    for c in range(5, num_attributes):
                        class_conf = output[0, c, d]
                        if class_conf > max_class_conf:
                            max_class_conf = class_conf
                            class_id = c - 5
                    
                    if class_id >= 0:
                        final_conf = obj_conf * max_class_conf
                        if final_conf > 0.5:  # ç¬¬äºŒå±‚è¿‡æ»¤
                            # è§£æè¾¹ç•Œæ¡†
                            x_center = output[0, 0, d]
                            y_center = output[0, 1, d]
                            w_norm = output[0, 2, d]
                            h_norm = output[0, 3, d]
                            
                            # è½¬æ¢ä¸ºåƒç´ åæ ‡
                            x1 = (x_center - w_norm/2) * width
                            y1 = (y_center - h_norm/2) * height
                            w = w_norm * width
                            h = h_norm * height
                            
                            detections.append({
                                'bbox': [x1, y1, w, h],
                                'confidence': final_conf,
                                'class_id': class_id,
                                'class_name': f'Class_{class_id}'
                            })
        
        elif analysis['detected_format'] == 'yolov8_format2':
            # YOLOv8æ ¼å¼2: [batch, num_detections, attributes]
            output = outputs[0]
            batch_size, num_detections, num_attributes = output.shape
            
            for d in range(min(num_detections, 1000)):
                obj_conf = output[0, d, 4]
                if obj_conf > 0.5:
                    class_id = -1
                    max_class_conf = 0
                    for c in range(5, num_attributes):
                        class_conf = output[0, d, c]
                        if class_conf > max_class_conf:
                            max_class_conf = class_conf
                            class_id = c - 5
                    
                    if class_id >= 0:
                        final_conf = obj_conf * max_class_conf
                        if final_conf > 0.5:
                            x_center = output[0, d, 0]
                            y_center = output[0, d, 1]
                            w_norm = output[0, d, 2]
                            h_norm = output[0, d, 3]
                            
                            x1 = (x_center - w_norm/2) * width
                            y1 = (y_center - h_norm/2) * height
                            w = w_norm * width
                            h = h_norm * height
                            
                            detections.append({
                                'bbox': [x1, y1, w, h],
                                'confidence': final_conf,
                                'class_id': class_id,
                                'class_name': f'Class_{class_id}'
                            })
        
        return detections
    
    def generate_cpp_deployment_info(self) -> Dict[str, Any]:
        """
        ç”ŸæˆC++éƒ¨ç½²å‚è€ƒä¿¡æ¯
        
        Returns:
            C++ä»£ç ä¿®æ”¹å»ºè®®
        """
        cpp_info = {
            'headers': [],
            'preprocessing': {},
            'model_loading': {},
            'inference': {},
            'postprocessing': {},
            'code_examples': {}
        }
        
        # ç”Ÿæˆå¤´æ–‡ä»¶ä¿¡æ¯
        cpp_info['headers'] = [
            '#include <opencv2/dnn/dnn.hpp>',
            '#include <onnxruntime/core/session/onnxruntime_cxx_api.h>',
            '#include <vector>',
            '#include <string>'
        ]
        
        # é¢„å¤„ç†å»ºè®®
        input_shape = self.input_info[self.input_names[0]]['shape']
        cpp_info['preprocessing'] = {
            'description': 'æ ¹æ®åˆ†æç»“æœç”Ÿæˆçš„å›¾åƒé¢„å¤„ç†ä»£ç ',
            'suggested_code': f'''
// å›¾åƒé¢„å¤„ç† (åŸºäºåˆ†æç»“æœ)
cv::Mat preprocess_image(const cv::Mat& image) {{
    // è°ƒæ•´å°ºå¯¸åˆ°æ¨¡å‹è¾“å…¥å°ºå¯¸
    cv::Mat resized;
    cv::resize(image, resized, cv::Size({input_shape[3]}, {input_shape[2]}));
    
    // è½¬æ¢ä¸ºæµ®ç‚¹ç±»å‹å¹¶å½’ä¸€åŒ–
    resized.convertTo(resized, CV_32F);
    resized = resized / 255.0f;
    
    // é€šé“è½¬æ¢ (BGR to RGB) å’Œç»´åº¦è°ƒæ•´
    cv::cvtColor(resized, resized, cv::COLOR_BGR2RGB);
    cv::dnn::blobFromImage(resized, resized, 1.0, cv::Size(), cv::Scalar(), false, false);
    
    return resized;
}}'''
        }
        
        # æ¨¡å‹åŠ è½½å»ºè®®
        cpp_info['model_loading'] = {
            'description': 'ONNXæ¨¡å‹åŠ è½½ä»£ç ',
            'suggested_code': f'''
// ONNXæ¨¡å‹åŠ è½½
ort::SessionOptions session_options;
session_options.SetInterOpNumThreads(1);
session_options.SetIntraOpNumThreads(1);

std::unique_ptr<ort::Session> session;
try {{
    session.reset(new ort::Session(env, "{self.model_path}", session_options));
    std::cout << "æ¨¡å‹åŠ è½½æˆåŠŸ" << std::endl;
}} catch (const std::exception& e) {{
    std::cerr << "æ¨¡å‹åŠ è½½å¤±è´¥: " << e.what() << std::endl;
}}'''
        }
        
        # æ¨ç†å»ºè®®
        cpp_info['inference'] = {
            'description': 'æ¨¡å‹æ¨ç†ä»£ç ',
            'suggested_code': '''
// æ¨¡å‹æ¨ç†
std::vector<Ort::Value> input_tensors;
input_tensors.push_back(Ort::Value::CreateTensor<float>(
    memory_info, input_data.data(), input_size,
    input_shape.data(), input_shape.size()));

auto output_tensors = session->Run(Ort::RunOptions{nullptr},
    input_names.data(), input_tensors.data(), 1,
    output_names.data(), output_names.size());'''
        }
        
        # åå¤„ç†å»ºè®®
        if self.output_info:
            output_shape = list(self.output_info[self.output_names[0]]['shape'].values())
            cpp_info['postprocessing'] = {
                'description': 'åŸºäºæ£€æµ‹ç»“æœçš„åå¤„ç†ä»£ç ',
                'detected_format': analysis.get('detected_format', 'unknown'),
                'suggested_code': '''
// YOLOæ£€æµ‹ç»“æœåå¤„ç† (ç¤ºä¾‹)
std::vector<DetectionResult> postprocess_yolo(
    const std::vector<Ort::Value>& output_tensors,
    float conf_threshold = 0.5f,
    float nms_threshold = 0.4f) {
    
    std::vector<DetectionResult> results;
    // æ ¹æ®å®é™…è¾“å‡ºæ ¼å¼å®ç°å…·ä½“çš„è§£æé€»è¾‘
    // å‚è§ä¸Šæ–¹analyze_yolo_output()çš„åˆ†æç»“æœ
    
    return results;
}'''
            }
        
        return cpp_info
    
    def print_summary(self):
        """æ‰“å°æ¨¡å‹æ‘˜è¦ä¿¡æ¯"""
        print("\n" + "="*60)
        print("ğŸ“‹ ONNXæ¨¡å‹åˆ†ææ‘˜è¦")
        print("="*60)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {self.model_path}")
        print(f"ğŸ”§ æ‰§è¡Œæä¾›è€…: {', '.join(self.session.get_providers())}")
        
        # è¾“å…¥ä¿¡æ¯
        print(f"\nğŸ“¥ è¾“å…¥èŠ‚ç‚¹ ({len(self.input_names)}ä¸ª):")
        for name in self.input_names:
            info = self.input_info[name]
            print(f"  - {name}: {info['shape']} ({info['dtype']})")
        
        # è¾“å‡ºä¿¡æ¯
        print(f"\nğŸ“¤ è¾“å‡ºèŠ‚ç‚¹ ({len(self.output_names)}ä¸ª):")
        for name in self.output_names:
            info = self.output_info[name]
            print(f"  - {name}: {info['shape']} ({info['dtype']})")
        
        print("="*60)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ONNXæ¨¡å‹ä¿¡æ¯åˆ†æå·¥å…·')
    parser.add_argument('--model', type=str, required=True,
                        help='ONNXæ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--image', type=str, required=True,
                        help='æµ‹è¯•å›¾åƒæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, default='analysis_result.jpg',
                        help='è¾“å‡ºç»“æœå›¾åƒè·¯å¾„')
    parser.add_argument('--conf_threshold', type=float, default=0.5,
                        help='ç½®ä¿¡åº¦é˜ˆå€¼')
    parser.add_argument('--save_json', type=str, default='model_analysis.json',
                        help='ä¿å­˜åˆ†æç»“æœåˆ°JSONæ–‡ä»¶')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.model):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model}")
        sys.exit(1)
    
    if not os.path.exists(args.image):
        print(f"âŒ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {args.image}")
        sys.exit(1)
    
    try:
        # åˆ›å»ºåˆ†æå™¨
        analyzer = ONNXModelAnalyzer(args.model)
        
        # æ‰“å°æ‘˜è¦
        analyzer.print_summary()
        
        # é¢„å¤„ç†å›¾åƒ
        input_data, original_size = analyzer.preprocess_image(args.image)
        
        # è¿è¡Œæ¨ç†
        outputs = analyzer.run_inference(input_data)
        
        # åˆ†æè¾“å‡º
        analysis = analyzer.analyze_yolo_output(outputs)
        
        # å¯è§†åŒ–ç»“æœ
        detections, analysis = analyzer.visualize_results(
            args.image, outputs, args.output, args.conf_threshold)
        
        # ç”ŸæˆC++éƒ¨ç½²ä¿¡æ¯
        cpp_info = analyzer.generate_cpp_deployment_info()
        
        # ä¿å­˜å®Œæ•´åˆ†æç»“æœ
        complete_analysis = {
            'model_info': analyzer.get_model_info(),
            'yolo_analysis': analysis,
            'detections': detections,
            'cpp_deployment_info': cpp_info
        }
        
        with open(args.save_json, 'w', encoding='utf-8') as f:
            json.dump(complete_analysis, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ å®Œæ•´åˆ†æç»“æœå·²ä¿å­˜åˆ°: {args.save_json}")
        
        # æ‰“å°C++éƒ¨ç½²å»ºè®®
        print(f"\nğŸ”§ C++ä»£ç éƒ¨ç½²å»ºè®®:")
        print("="*40)
        print(cpp_info['preprocessing']['description'])
        print(cpp_info['preprocessing']['suggested_code'])
        
        print(f"\nğŸ“‹ å»ºè®®ä¿®æ”¹çš„æ–‡ä»¶:")
        print("  - DataProcessor.cpp: æ·»åŠ æ¨¡å‹é¢„å¤„ç†é€»è¾‘")
        print("  - DLProcessor.cpp: æ ¹æ®è¾“å‡ºæ ¼å¼ä¿®æ”¹PostProcessYoloæ–¹æ³•")
        print("  - ç¡®ä¿æ­£ç¡®è®¾ç½®è¾“å…¥å¼ é‡å’Œåå¤„ç†å‚æ•°")
        
        print(f"\nâœ… åˆ†æå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()